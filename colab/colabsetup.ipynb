{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz_JGUordLoQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "941c50050e324d789fbf8b38f0c0884a",
            "fa213537caab43a5b5445f0d550188f4",
            "de499fa5bf3c44aaab45199fe3c70822",
            "9353ab3c21ed487fbcdd555378137599",
            "81dacb2f162c4439bc4aec2eb932dc7f"
          ]
        },
        "id": "BzEGAvFWd1F5",
        "outputId": "69b87956-f830-4755-b174-1b76855cc78d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,765 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,017 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,984 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,246 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,742 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,553 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,295 kB]\n",
            "Fetched 23.0 MB in 2s (9,499 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8 [186 kB]\n",
            "Fetched 186 kB in 0s (451 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126111 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.8_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "ğŸš€ Using device: cuda\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 14 GB\n",
            "ğŸ” Starting FIXED question extraction...\n",
            "Text extraction error: [Errno 2] No such file or directory: '/content/drive/MyDrive/Os question bank.pdf'\n",
            "âœ… Text extraction found: 0 questions\n",
            "ğŸ” Enhancing with OCR...\n",
            "OCR extraction error: Unable to get page count.\n",
            "I/O Error: Couldn't open file '/content/drive/MyDrive/Os question bank.pdf': No such file or directory.\n",
            "\n",
            "âœ… OCR extraction found: 0 questions\n",
            "âœ… After deduplication: 0 unique questions\n",
            "\n",
            "ğŸ“Š QUESTION EXTRACTION SUMMARY\n",
            "Total unique questions extracted: 0\n",
            "\n",
            "ğŸ” Starting topic extraction for all units...\n",
            "Text extraction error for topics: [Errno 2] No such file or directory: '/content/drive/MyDrive/os syallabus.pdf'\n",
            "ğŸ” Text extraction insufficient, trying OCR...\n",
            "OCR extraction error for topics: Unable to get page count.\n",
            "I/O Error: Couldn't open file '/content/drive/MyDrive/os syallabus.pdf': No such file or directory.\n",
            "\n",
            "\n",
            "ğŸ“Š TOPIC EXTRACTION SUMMARY\n",
            "Total units found: 0\n",
            "Total topics extracted: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "941c50050e324d789fbf8b38f0c0884a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa213537caab43a5b5445f0d550188f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de499fa5bf3c44aaab45199fe3c70822",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9353ab3c21ed487fbcdd555378137599",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81dacb2f162c4439bc4aec2eb932dc7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš ï¸ Model loading failed: [Errno 2] No such file or directory: '/content/drive/MyDrive/roberta_multitask_model_full.pt'\n",
            "Using dummy predictor for demonstration...\n",
            "\n",
            "ğŸ”„ Preparing GPU-optimized topic matching for 0 topics...\n",
            "âŒ No questions found to process\n",
            "\n",
            "ğŸ¯ FIXED Process completed! Check the generated CSV files for results.\n",
            "ğŸ“ Output files:\n",
            "  - FIXED_processed_questions.csv (main results)\n",
            "  - FIXED_unit_wise_topics.csv (unit-wise topics)\n",
            "ğŸ§¹ GPU memory cleaned up\n",
            "\n",
            "================================================================================\n",
            "ğŸ”§ KEY FIXES IMPLEMENTED:\n",
            "================================================================================\n",
            "1. âœ… FIXED Question Number Removal:\n",
            "   - Removes '1.', 'Q1.', '1)', etc. from questions\n",
            "   - Clean questions without numbering artifacts\n",
            "\n",
            "2. âœ… ENHANCED Question Detection:\n",
            "   - Better patterns for both '?' and '.' endings\n",
            "   - Improved validation for command-style questions\n",
            "   - More comprehensive question indicators\n",
            "\n",
            "3. âœ… IMPROVED Text Processing:\n",
            "   - Better line filtering to skip headers/metadata\n",
            "   - Enhanced cleaning of question text\n",
            "   - Reduced false positives\n",
            "\n",
            "4. âœ… ROBUST Extraction Pipeline:\n",
            "   - Multiple extraction strategies\n",
            "   - Better OCR fallback\n",
            "   - Improved deduplication\n",
            "\n",
            "5. âœ… INTEGER MARKS with MINIMUM 3:\n",
            "   - All marks stored as integers (3, 4, 5, 6, 7, 8)\n",
            "   - Minimum mark value enforced to be 3\n",
            "   - No decimal marks or marks below 3\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“š GPU-Optimized Question Analysis Pipeline - FIXED Version\n",
        "# ----------------------------------------------------\n",
        "# Fixes: Better question extraction + Remove question numbers\n",
        "\n",
        "# âœ… STEP 0: Install Required Libraries\n",
        "!pip install transformers sentence-transformers pdfplumber torch torchvision torchaudio pytesseract Pillow pdf2image --quiet\n",
        "!apt-get update && apt-get install -y tesseract-ocr poppler-utils\n",
        "\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from IPython.display import display\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "import hashlib\n",
        "\n",
        "# âœ… GPU Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ğŸš€ Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")\n",
        "\n",
        "# âœ… STEP 1: FIXED Question Extraction\n",
        "question_pdf_path = \"/content/drive/MyDrive/Os question bank.pdf\"\n",
        "\n",
        "def extract_questions_fixed(pdf_path):\n",
        "    \"\"\"\n",
        "    FIXED: Better question extraction for all formats\n",
        "    \"\"\"\n",
        "    print(\"ğŸ” Starting FIXED question extraction...\")\n",
        "\n",
        "    # Try text extraction first\n",
        "    questions_text = extract_questions_from_text_fixed(pdf_path)\n",
        "    print(f\"âœ… Text extraction found: {len(questions_text)} questions\")\n",
        "\n",
        "    # If insufficient, try OCR\n",
        "    if len(questions_text) < 50:\n",
        "        print(\"ğŸ” Enhancing with OCR...\")\n",
        "        questions_ocr = extract_questions_from_ocr_fixed(pdf_path)\n",
        "        print(f\"âœ… OCR extraction found: {len(questions_ocr)} questions\")\n",
        "\n",
        "        # Combine and deduplicate\n",
        "        all_questions = questions_text + questions_ocr\n",
        "        questions_text = remove_exact_duplicates(all_questions)\n",
        "\n",
        "    print(f\"âœ… After deduplication: {len(questions_text)} unique questions\")\n",
        "    return questions_text\n",
        "\n",
        "def extract_questions_from_text_fixed(pdf_path):\n",
        "    \"\"\"FIXED text extraction with better patterns\"\"\"\n",
        "    questions = []\n",
        "\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page_num, page in enumerate(pdf.pages):\n",
        "                text = page.extract_text()\n",
        "                if text:\n",
        "                    page_questions = process_page_text_fixed(text, page_num + 1)\n",
        "                    questions.extend(page_questions)\n",
        "                    print(f\"ğŸ“„ Page {page_num + 1}: Found {len(page_questions)} questions\")\n",
        "    except Exception as e:\n",
        "        print(f\"Text extraction error: {e}\")\n",
        "\n",
        "    return questions\n",
        "\n",
        "def process_page_text_fixed(text, page_num):\n",
        "    \"\"\"FIXED: Comprehensive question detection\"\"\"\n",
        "    questions = []\n",
        "\n",
        "    # Split into lines and clean\n",
        "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
        "\n",
        "    # Process each line\n",
        "    for line in lines:\n",
        "        # Skip headers and unwanted content\n",
        "        if should_skip_line(line):\n",
        "            continue\n",
        "\n",
        "        # Extract question from line\n",
        "        question = extract_question_from_line(line)\n",
        "        if question:\n",
        "            questions.append(question)\n",
        "\n",
        "    return questions\n",
        "\n",
        "def should_skip_line(line):\n",
        "    \"\"\"Check if line should be skipped\"\"\"\n",
        "    skip_patterns = [\n",
        "        'research methodology', 'intellectual property rights', 'operating system',\n",
        "        'unit i:', 'unit ii:', 'unit iii:', 'unit iv:', 'unit v:',\n",
        "        'unique questions', '80 unique', 'question bank', 'course code',\n",
        "        'credits:', 'contact hours', 'page', 'syllabus', 'introduction',\n",
        "        'method of data collection', 'research design'\n",
        "    ]\n",
        "\n",
        "    line_lower = line.lower()\n",
        "    return any(pattern in line_lower for pattern in skip_patterns)\n",
        "\n",
        "def extract_question_from_line(line):\n",
        "    \"\"\"FIXED: Extract clean question from line\"\"\"\n",
        "    original_line = line.strip()\n",
        "\n",
        "    # Remove question numbers at the beginning\n",
        "    line = remove_question_numbers(line)\n",
        "\n",
        "    # Check if it's a valid question\n",
        "    if not is_valid_question_fixed(line):\n",
        "        return None\n",
        "\n",
        "    # Clean the question\n",
        "    clean_question = clean_question_text_fixed(line)\n",
        "\n",
        "    return clean_question\n",
        "\n",
        "def remove_question_numbers(line):\n",
        "    \"\"\"FIXED: Remove question numbers from beginning\"\"\"\n",
        "    # Patterns to remove question numbers\n",
        "    patterns = [\n",
        "        r'^\\d+\\.\\s*',           # 1.\n",
        "        r'^Q\\d+\\.\\s*',          # Q1.\n",
        "        r'^\\d+\\)\\s*',           # 1)\n",
        "        r'^\\d+\\s+',             # 1 (followed by space)\n",
        "        r'^[IVX]+\\.\\s*',        # I. II. III.\n",
        "        r'^\\([a-z]\\)\\s*',       # (a) (b) (c)\n",
        "        r'^\\w+\\s*:\\s*',         # Unit I: etc\n",
        "    ]\n",
        "\n",
        "    cleaned_line = line\n",
        "    for pattern in patterns:\n",
        "        cleaned_line = re.sub(pattern, '', cleaned_line).strip()\n",
        "\n",
        "    return cleaned_line\n",
        "\n",
        "def is_valid_question_fixed(text):\n",
        "    \"\"\"FIXED: Better question validation\"\"\"\n",
        "    if not text or len(text.strip()) < 5:\n",
        "        return False\n",
        "\n",
        "    text = text.strip()\n",
        "\n",
        "    # Must end with question mark OR period\n",
        "    if not (text.endswith('?') or text.endswith('.')):\n",
        "        return False\n",
        "\n",
        "    # Must have minimum words\n",
        "    words = text.split()\n",
        "    if len(words) < 3:\n",
        "        return False\n",
        "\n",
        "    # Skip if too long (likely paragraph)\n",
        "    if len(words) > 25:\n",
        "        return False\n",
        "\n",
        "    # Check for question indicators\n",
        "    question_indicators = [\n",
        "        # Question words\n",
        "        'what', 'how', 'why', 'when', 'where', 'which', 'who',\n",
        "        # Command words\n",
        "        'define', 'explain', 'describe', 'list', 'discuss', 'mention',\n",
        "        'compare', 'differentiate', 'distinguish', 'analyze', 'examine',\n",
        "        'illustrate', 'evaluate', 'assess', 'write', 'draw', 'demonstrate',\n",
        "        'simulate', 'apply', 'implement', 'solve', 'calculate', 'translate',\n",
        "        'design', 'create', 'build', 'develop', 'construct', 'model'\n",
        "    ]\n",
        "\n",
        "    first_words = ' '.join(words[:3]).lower()\n",
        "    has_indicator = any(indicator in first_words for indicator in question_indicators)\n",
        "\n",
        "    # For statements ending with period, they should have command words\n",
        "    if text.endswith('.'):\n",
        "        first_word = words[0].lower() if words else \"\"\n",
        "        command_words = ['define', 'explain', 'describe', 'list', 'discuss', 'mention',\n",
        "                        'compare', 'differentiate', 'distinguish', 'analyze', 'examine',\n",
        "                        'illustrate', 'evaluate', 'assess', 'write', 'draw', 'demonstrate',\n",
        "                        'simulate', 'apply', 'implement', 'solve', 'calculate', 'translate',\n",
        "                        'design', 'create', 'build', 'develop', 'construct', 'model']\n",
        "\n",
        "        if first_word in command_words:\n",
        "            has_indicator = True\n",
        "\n",
        "    return has_indicator\n",
        "\n",
        "def clean_question_text_fixed(text):\n",
        "    \"\"\"FIXED: Clean question text\"\"\"\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Ensure proper capitalization\n",
        "    if text and text[0].islower():\n",
        "        text = text[0].upper() + text[1:]\n",
        "\n",
        "    # Ensure proper ending\n",
        "    if text.endswith('?'):\n",
        "        text = text.rstrip('?') + '?'\n",
        "    elif text.endswith('.'):\n",
        "        text = text.rstrip('.') + '.'\n",
        "\n",
        "    # Remove any remaining artifacts\n",
        "    text = re.sub(r'^[^\\w\\s]*', '', text)  # Remove leading non-word chars\n",
        "\n",
        "    return text\n",
        "\n",
        "def extract_questions_from_ocr_fixed(pdf_path):\n",
        "    \"\"\"FIXED OCR extraction\"\"\"\n",
        "    questions = []\n",
        "\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path, dpi=300)\n",
        "        for i, image in enumerate(images):\n",
        "            print(f\"ğŸ“„ OCR processing page {i+1}/{len(images)}\")\n",
        "\n",
        "            # Enhanced OCR configuration\n",
        "            custom_config = r'--oem 3 --psm 6'\n",
        "            ocr_text = pytesseract.image_to_string(image, config=custom_config)\n",
        "\n",
        "            page_questions = process_page_text_fixed(ocr_text, i + 1)\n",
        "            questions.extend(page_questions)\n",
        "    except Exception as e:\n",
        "        print(f\"OCR extraction error: {e}\")\n",
        "\n",
        "    return questions\n",
        "\n",
        "def remove_exact_duplicates(questions):\n",
        "    \"\"\"Remove exact duplicates while preserving order\"\"\"\n",
        "    seen = set()\n",
        "    unique_questions = []\n",
        "\n",
        "    for question in questions:\n",
        "        # Normalize for comparison\n",
        "        normalized = question.lower().strip()\n",
        "        normalized = re.sub(r'\\s+', ' ', normalized)\n",
        "        normalized = re.sub(r'[^\\w\\s]', '', normalized)\n",
        "\n",
        "        if normalized and normalized not in seen and len(normalized) > 5:\n",
        "            seen.add(normalized)\n",
        "            unique_questions.append(question)\n",
        "\n",
        "    return unique_questions\n",
        "\n",
        "# Extract questions\n",
        "questions = extract_questions_fixed(question_pdf_path)\n",
        "print(f\"\\nğŸ“Š QUESTION EXTRACTION SUMMARY\")\n",
        "print(f\"Total unique questions extracted: {len(questions)}\")\n",
        "\n",
        "if questions:\n",
        "    print(\"\\nğŸ“‹ Sample questions:\")\n",
        "    for i, q in enumerate(questions[:10]):\n",
        "        print(f\"{i+1}. {q}\")\n",
        "\n",
        "# âœ… STEP 2: Topic Extraction (Already working well)\n",
        "syllabus_pdf_path = \"/content/drive/MyDrive/os syallabus.pdf\"\n",
        "\n",
        "def extract_topics_all_units_final(pdf_path):\n",
        "    \"\"\"Topic extraction - keeping existing working version\"\"\"\n",
        "    print(\"\\nğŸ” Starting topic extraction for all units...\")\n",
        "\n",
        "    # Try text extraction first\n",
        "    unit_topics_text = extract_topics_from_text_all_units_final(pdf_path)\n",
        "\n",
        "    # If insufficient, try OCR\n",
        "    total_topics = sum(len(topics) for topics in unit_topics_text.values())\n",
        "    if total_topics < 20:\n",
        "        print(\"ğŸ” Text extraction insufficient, trying OCR...\")\n",
        "        unit_topics_ocr = extract_topics_from_ocr_all_units_final(pdf_path)\n",
        "\n",
        "        # Merge results\n",
        "        for unit, topics in unit_topics_ocr.items():\n",
        "            if unit in unit_topics_text:\n",
        "                unit_topics_text[unit].extend(topics)\n",
        "            else:\n",
        "                unit_topics_text[unit] = topics\n",
        "\n",
        "    # Clean and deduplicate\n",
        "    cleaned_unit_topics = {}\n",
        "    for unit, topics in unit_topics_text.items():\n",
        "        cleaned_topics = clean_and_deduplicate_topics(topics)\n",
        "        if cleaned_topics:\n",
        "            cleaned_unit_topics[unit] = cleaned_topics\n",
        "\n",
        "    return cleaned_unit_topics\n",
        "\n",
        "def extract_topics_from_text_all_units_final(pdf_path):\n",
        "    \"\"\"Final text extraction for all units\"\"\"\n",
        "    unit_topics = defaultdict(list)\n",
        "\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            full_text = \"\"\n",
        "            for page in pdf.pages:\n",
        "                text = page.extract_text()\n",
        "                if text:\n",
        "                    full_text += text + \"\\n\"\n",
        "\n",
        "        if full_text.strip():\n",
        "            unit_topics = parse_all_units_final(full_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Text extraction error for topics: {e}\")\n",
        "\n",
        "    return unit_topics\n",
        "\n",
        "def extract_topics_from_ocr_all_units_final(pdf_path):\n",
        "    \"\"\"Final OCR extraction for all units\"\"\"\n",
        "    unit_topics = defaultdict(list)\n",
        "\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path, dpi=300)\n",
        "        full_ocr_text = \"\"\n",
        "\n",
        "        for i, image in enumerate(images):\n",
        "            print(f\"ğŸ“„ OCR processing page {i+1}/{len(images)} for topics\")\n",
        "\n",
        "            ocr_text = pytesseract.image_to_string(image, lang='eng')\n",
        "            full_ocr_text += ocr_text + \"\\n\"\n",
        "\n",
        "        if full_ocr_text.strip():\n",
        "            unit_topics = parse_all_units_final(full_ocr_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"OCR extraction error for topics: {e}\")\n",
        "\n",
        "    return unit_topics\n",
        "\n",
        "def parse_all_units_final(text):\n",
        "    \"\"\"Final parsing to capture all units\"\"\"\n",
        "    unit_topics = defaultdict(list)\n",
        "\n",
        "    # Remove unwanted content first\n",
        "    text = remove_unwanted_content_final(text)\n",
        "\n",
        "    # Strategy 1: Standard unit parsing\n",
        "    unit_patterns = [\n",
        "        r'Unit\\s+([IVX\\d]+)[:\\s]*\\n(.*?)(?=Unit\\s+[IVX\\d]+|Course\\s+Outcomes|Suggested\\s+Learning|Text\\s+Books|Reference\\s+Books|$)',\n",
        "        r'Unit\\s+([IVX\\d]+)[:\\s]*(.*?)(?=Unit\\s+[IVX\\d]+|Course\\s+Outcomes|Suggested\\s+Learning|Text\\s+Books|Reference\\s+Books|$)',\n",
        "        r'Unit\\s+([IVX\\d]+)\\s*\\n(.*?)(?=Unit\\s+[IVX\\d]+|Course\\s+Outcomes|$)'\n",
        "    ]\n",
        "\n",
        "    # Try each pattern\n",
        "    for pattern_idx, pattern in enumerate(unit_patterns):\n",
        "        unit_matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "        if unit_matches and len(unit_matches) >= 2:\n",
        "            print(f\"âœ… Using pattern {pattern_idx + 1}: Found {len(unit_matches)} units\")\n",
        "\n",
        "            for unit_num, unit_content in unit_matches:\n",
        "                unit_key = f\"Unit {unit_num}\"\n",
        "                topics = extract_topics_from_unit_content_final(unit_content)\n",
        "\n",
        "                if topics:\n",
        "                    unit_topics[unit_key].extend(topics)\n",
        "                    print(f\"  {unit_key}: {len(topics)} topics\")\n",
        "\n",
        "            if len(unit_topics) >= 2:\n",
        "                break\n",
        "\n",
        "    # Strategy 2: If standard parsing failed, try line-by-line approach\n",
        "    if len(unit_topics) < 2:\n",
        "        print(\"ğŸ” Trying alternative unit detection...\")\n",
        "        unit_topics = parse_units_line_by_line(text)\n",
        "\n",
        "    return dict(unit_topics)\n",
        "\n",
        "def parse_units_line_by_line(text):\n",
        "    \"\"\"Alternative line-by-line parsing approach\"\"\"\n",
        "    unit_topics = defaultdict(list)\n",
        "    current_unit = None\n",
        "    current_content = []\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        # Check if line is a unit header\n",
        "        unit_match = re.match(r'Unit\\s+([IVX\\d]+)', line, re.IGNORECASE)\n",
        "        if unit_match:\n",
        "            # Save previous unit content\n",
        "            if current_unit and current_content:\n",
        "                content_text = '\\n'.join(current_content)\n",
        "                topics = extract_topics_from_unit_content_final(content_text)\n",
        "                if topics:\n",
        "                    unit_topics[current_unit].extend(topics)\n",
        "                    print(f\"  {current_unit}: {len(topics)} topics\")\n",
        "\n",
        "            # Start new unit\n",
        "            current_unit = f\"Unit {unit_match.group(1)}\"\n",
        "            current_content = []\n",
        "\n",
        "        # Check if we've reached end sections\n",
        "        elif any(end_marker in line for end_marker in ['Course Outcomes', 'Suggested Learning',\n",
        "                                                      'Text Books', 'Reference Books']):\n",
        "            # Save final unit content\n",
        "            if current_unit and current_content:\n",
        "                content_text = '\\n'.join(current_content)\n",
        "                topics = extract_topics_from_unit_content_final(content_text)\n",
        "                if topics:\n",
        "                    unit_topics[current_unit].extend(topics)\n",
        "                    print(f\"  {current_unit}: {len(topics)} topics\")\n",
        "            break\n",
        "\n",
        "        # Add line to current unit content\n",
        "        elif current_unit and line:\n",
        "            current_content.append(line)\n",
        "\n",
        "    # Don't forget the last unit\n",
        "    if current_unit and current_content:\n",
        "        content_text = '\\n'.join(current_content)\n",
        "        topics = extract_topics_from_unit_content_final(content_text)\n",
        "        if topics:\n",
        "            unit_topics[current_unit].extend(topics)\n",
        "            print(f\"  {current_unit}: {len(topics)} topics\")\n",
        "\n",
        "    return dict(unit_topics)\n",
        "\n",
        "def remove_unwanted_content_final(text):\n",
        "    \"\"\"Remove unwanted content\"\"\"\n",
        "    unwanted_patterns = [\n",
        "        r'Pedagogy[:/].*?(?=\\n\\n|\\nUnit|\\n[A-Z])',\n",
        "        r'Links[:/].*?(?=\\n\\n|\\nUnit|\\n[A-Z])',\n",
        "        r'Impartus.*?(?=\\n\\n|\\nUnit|\\n[A-Z])',\n",
        "        r'https?://[^\\s]+',\n",
        "        r'Course\\s+Code[:/].*?(?=\\n)',\n",
        "        r'Credits[:/].*?(?=\\n)',\n",
        "        r'Contact\\s+Hours[:/].*?(?=\\n)',\n",
        "        r'Course\\s+Coordinator[:/].*?(?=\\n)',\n",
        "        r'Prerequisites?[:/].*?(?=\\n)',\n",
        "        r'Pre\\s*â€“\\s*requisites[:/].*?(?=\\n)',\n",
        "    ]\n",
        "\n",
        "    cleaned_text = text\n",
        "    for pattern in unwanted_patterns:\n",
        "        cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "def extract_topics_from_unit_content_final(content):\n",
        "    \"\"\"Extract topics from unit content\"\"\"\n",
        "    topics = []\n",
        "\n",
        "    # Multiple splitting strategies\n",
        "    split_patterns = [\n",
        "        r'[;\\n]',           # Semicolon or newline\n",
        "        r'[,\\n]',           # Comma or newline\n",
        "        r'[\\n]'             # Just newline\n",
        "    ]\n",
        "\n",
        "    for pattern in split_patterns:\n",
        "        potential_topics = re.split(pattern, content)\n",
        "        temp_topics = []\n",
        "\n",
        "        for topic in potential_topics:\n",
        "            topic = topic.strip()\n",
        "\n",
        "            # Clean the topic\n",
        "            topic = re.sub(r'^[â€¢\\-\\*\\d\\.\\s()]+', '', topic)\n",
        "            topic = re.sub(r'^\\w+\\s*:', '', topic)\n",
        "            topic = re.sub(r'\\s+', ' ', topic).strip()\n",
        "\n",
        "            # Quality checks\n",
        "            if is_valid_topic_final(topic):\n",
        "                temp_topics.append(topic)\n",
        "\n",
        "        # Use the pattern that gives most topics\n",
        "        if len(temp_topics) > len(topics):\n",
        "            topics = temp_topics\n",
        "\n",
        "    return topics\n",
        "\n",
        "def is_valid_topic_final(topic):\n",
        "    \"\"\"Topic validation\"\"\"\n",
        "    if not topic or len(topic) < 4:\n",
        "        return False\n",
        "\n",
        "    # Length checks\n",
        "    if len(topic) > 120 or len(topic) < 4:\n",
        "        return False\n",
        "\n",
        "    # Skip all caps\n",
        "    if topic.isupper() and len(topic) > 15:\n",
        "        return False\n",
        "\n",
        "    # Unwanted keywords\n",
        "    unwanted_keywords = [\n",
        "        'pedagogy', 'delivery', 'tools', 'chalk', 'talk', 'powerpoint',\n",
        "        'video', 'link', 'nptel', 'impartus', 'recording', 'course code',\n",
        "        'credit', 'contact hour', 'coordinator', 'prerequisite'\n",
        "    ]\n",
        "\n",
        "    topic_lower = topic.lower()\n",
        "    if any(keyword in topic_lower for keyword in unwanted_keywords):\n",
        "        return False\n",
        "\n",
        "    # Must have reasonable word count\n",
        "    word_count = len(topic.split())\n",
        "    if word_count < 2 or word_count > 15:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def clean_and_deduplicate_topics(topics):\n",
        "    \"\"\"Clean and deduplicate topics\"\"\"\n",
        "    if not topics:\n",
        "        return []\n",
        "\n",
        "    cleaned_topics = []\n",
        "    seen_normalized = set()\n",
        "\n",
        "    for topic in topics:\n",
        "        # Normalize for comparison\n",
        "        normalized = topic.lower().strip()\n",
        "        normalized = re.sub(r'\\s+', ' ', normalized)\n",
        "        normalized = re.sub(r'[^\\w\\s]', '', normalized)\n",
        "\n",
        "        if normalized and normalized not in seen_normalized and len(normalized) > 3:\n",
        "            seen_normalized.add(normalized)\n",
        "            cleaned_topics.append(topic.strip())\n",
        "\n",
        "    return cleaned_topics\n",
        "\n",
        "# Extract topics by units\n",
        "unit_topics = extract_topics_all_units_final(syllabus_pdf_path)\n",
        "\n",
        "print(f\"\\nğŸ“Š TOPIC EXTRACTION SUMMARY\")\n",
        "total_topics = sum(len(topics) for topics in unit_topics.values())\n",
        "print(f\"Total units found: {len(unit_topics)}\")\n",
        "print(f\"Total topics extracted: {total_topics}\")\n",
        "\n",
        "for unit, topics in unit_topics.items():\n",
        "    print(f\"\\nğŸ“š {unit}: {len(topics)} topics\")\n",
        "    for i, topic in enumerate(topics[:5]):\n",
        "        print(f\"  {i+1}. {topic}\")\n",
        "    if len(topics) > 5:\n",
        "        print(f\"  ... and {len(topics)-5} more\")\n",
        "\n",
        "# âœ… STEP 3: GPU-Optimized RoBERTa Model (Keep existing)\n",
        "class RobertaMultiTask(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)\n",
        "        self.classifier = nn.Linear(self.roberta.config.hidden_size, 6)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_output = self.dropout(output.last_hidden_state[:, 0, :])\n",
        "        mark_pred = self.regressor(cls_output).squeeze(-1)\n",
        "        bloom_pred = self.classifier(cls_output)\n",
        "        return mark_pred, bloom_pred\n",
        "\n",
        "# Load model\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "model_path = \"/content/drive/MyDrive/roberta_multitask_model_full.pt\"\n",
        "\n",
        "try:\n",
        "    from torch.serialization import add_safe_globals\n",
        "    add_safe_globals({'RobertaMultiTask': RobertaMultiTask})\n",
        "    model = torch.load(model_path, map_location=device, weights_only=False)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"âœ… RoBERTa model loaded on {device}\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Model loading failed: {e}\")\n",
        "    print(\"Using dummy predictor for demonstration...\")\n",
        "    model = None\n",
        "\n",
        "# âœ… STEP 4: Prediction Functions (Keep existing)\n",
        "bloom_map = {0: \"L1\", 1: \"L2\", 2: \"L3\", 3: \"L4\", 4: \"L5\", 5: \"L6\"}\n",
        "\n",
        "def predict_question_gpu(question):\n",
        "    \"\"\"GPU-optimized prediction with integer marks and minimum 3\"\"\"\n",
        "    if model is None:\n",
        "        import random\n",
        "        marks = max(3, random.randint(3, 8))  # Integer marks, minimum 3\n",
        "        bloom = random.choice(list(bloom_map.values()))\n",
        "        return marks, bloom\n",
        "\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mark_pred, bloom_pred = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "        marks = max(3, round(mark_pred.cpu().item()))  # Integer marks, minimum 3\n",
        "        bloom = bloom_map[torch.argmax(bloom_pred).cpu().item()]\n",
        "    return marks, bloom\n",
        "\n",
        "def predict_batch_gpu(questions, batch_size=32):\n",
        "    \"\"\"GPU-optimized batch prediction with integer marks and minimum 3\"\"\"\n",
        "    if model is None:\n",
        "        import random\n",
        "        results = []\n",
        "        for _ in questions:\n",
        "            marks = max(3, random.randint(3, 8))  # Integer marks, minimum 3\n",
        "            bloom = random.choice(list(bloom_map.values()))\n",
        "            results.append((marks, bloom))\n",
        "        return results\n",
        "\n",
        "    results = []\n",
        "    for i in range(0, len(questions), batch_size):\n",
        "        batch = questions[i:i+batch_size]\n",
        "\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding='max_length',\n",
        "                          truncation=True, max_length=128)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mark_preds, bloom_preds = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "            marks = [max(3, round(mark.cpu().item())) for mark in mark_preds]  # Integer marks, minimum 3\n",
        "            blooms = [bloom_map[torch.argmax(bloom_pred).cpu().item()] for bloom_pred in bloom_preds]\n",
        "\n",
        "        results.extend(list(zip(marks, blooms)))\n",
        "\n",
        "    return results\n",
        "\n",
        "def get_difficulty(marks):\n",
        "    if marks <= 3:\n",
        "        return \"Easy\"\n",
        "    elif marks <= 6:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"Hard\"\n",
        "\n",
        "# âœ… STEP 5: Topic Matching (Keep existing)\n",
        "all_topics = []\n",
        "topic_to_unit = {}\n",
        "\n",
        "for unit, topics in unit_topics.items():\n",
        "    for topic in topics:\n",
        "        all_topics.append(topic)\n",
        "        topic_to_unit[topic] = unit\n",
        "\n",
        "print(f\"\\nğŸ”„ Preparing GPU-optimized topic matching for {len(all_topics)} topics...\")\n",
        "\n",
        "if len(all_topics) > 0:\n",
        "    sbert_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "    topic_embeddings = sbert_model.encode(all_topics, convert_to_tensor=True, device=device)\n",
        "\n",
        "    def match_topics_batch_gpu(questions, batch_size=32):\n",
        "        \"\"\"GPU-optimized batch topic matching\"\"\"\n",
        "        results = []\n",
        "        for i in range(0, len(questions), batch_size):\n",
        "            batch = questions[i:i+batch_size]\n",
        "\n",
        "            q_embeddings = sbert_model.encode(batch, convert_to_tensor=True, device=device)\n",
        "            scores = util.cos_sim(q_embeddings, topic_embeddings)\n",
        "\n",
        "            for j, question_scores in enumerate(scores):\n",
        "                best_topic_idx = torch.argmax(question_scores).cpu().item()\n",
        "                similarity_score = question_scores[best_topic_idx].cpu().item()\n",
        "\n",
        "                best_topic = all_topics[best_topic_idx]\n",
        "                unit = topic_to_unit[best_topic]\n",
        "\n",
        "                results.append((best_topic, unit, similarity_score))\n",
        "\n",
        "        return results\n",
        "else:\n",
        "    def match_topics_batch_gpu(questions, batch_size=32):\n",
        "        return [(\"No topics available\", \"Unknown Unit\", 0.0) for _ in questions]\n",
        "\n",
        "# âœ… STEP 6: Process Questions\n",
        "if len(questions) > 0:\n",
        "    print(f\"\\nğŸš€ Processing {len(questions)} questions...\")\n",
        "\n",
        "    batch_size = 32 if torch.cuda.is_available() else 16\n",
        "\n",
        "    print(\"ğŸ”„ Running batch predictions...\")\n",
        "    prediction_results = predict_batch_gpu(questions, batch_size)\n",
        "\n",
        "    print(\"ğŸ”„ Running batch topic matching...\")\n",
        "    topic_results = match_topics_batch_gpu(questions, batch_size)\n",
        "\n",
        "    # Combine results\n",
        "    results = []\n",
        "    for i, (question, (marks, bloom), (matched_topic, unit, similarity)) in enumerate(zip(questions, prediction_results, topic_results)):\n",
        "        difficulty = get_difficulty(marks)\n",
        "\n",
        "        results.append({\n",
        "            \"question_id\": i+1,\n",
        "            \"question\": question,\n",
        "            \"predicted_marks\": marks,\n",
        "            \"bloom_level\": bloom,\n",
        "            \"difficulty\": difficulty,\n",
        "            \"matched_topic\": matched_topic,\n",
        "            \"matched_unit\": unit,\n",
        "            \"topic_similarity\": round(similarity, 3)\n",
        "        })\n",
        "\n",
        "    # âœ… STEP 7: Save Results\n",
        "    if results:\n",
        "        # Save main results\n",
        "        df = pd.DataFrame(results)\n",
        "        output_file = \"FIXED_processed_questions.csv\"\n",
        "        df.to_csv(output_file, index=False)\n",
        "        print(f\"\\nâœ… Saved {len(results)} processed questions to '{output_file}'\")\n",
        "\n",
        "        # Save unit-wise topics\n",
        "        unit_topics_data = []\n",
        "        for unit, topics in unit_topics.items():\n",
        "            for i, topic in enumerate(topics, 1):\n",
        "                unit_topics_data.append({\n",
        "                    \"unit\": unit,\n",
        "                    \"topic_id\": i,\n",
        "                    \"topic\": topic\n",
        "                })\n",
        "\n",
        "        if unit_topics_data:\n",
        "            topics_df = pd.DataFrame(unit_topics_data)\n",
        "            topics_file = \"FIXED_unit_wise_topics.csv\"\n",
        "            topics_df.to_csv(topics_file, index=False)\n",
        "            print(f\"âœ… Saved {len(unit_topics_data)} topics to '{topics_file}'\")\n",
        "\n",
        "        # Display results\n",
        "        print(f\"\\nğŸ“Š FIXED PROCESSING SUMMARY\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Device used: {device}\")\n",
        "        print(f\"Questions extracted: {len(questions)}\")\n",
        "        print(f\"Questions processed: {len(results)}\")\n",
        "        print(f\"Units found: {len(unit_topics)}\")\n",
        "        print(f\"Total topics: {len(all_topics)}\")\n",
        "        print(f\"Success rate: {len(results)/len(questions)*100:.1f}%\")\n",
        "\n",
        "        print(f\"\\nğŸ“‹ SAMPLE RESULTS:\")\n",
        "        display(df.head(10))\n",
        "\n",
        "        print(f\"\\nğŸ“š UNIT-WISE TOPIC DISTRIBUTION:\")\n",
        "        if unit_topics_data:\n",
        "            unit_counts = topics_df['unit'].value_counts()\n",
        "            for unit, count in unit_counts.items():\n",
        "                print(f\"{unit}: {count} topics\")\n",
        "\n",
        "        print(f\"\\nğŸ“ˆ QUESTION STATISTICS:\")\n",
        "        print(\"Difficulty Distribution:\")\n",
        "        print(df['difficulty'].value_counts())\n",
        "        print(\"\\nBloom Level Distribution:\")\n",
        "        print(df['bloom_level'].value_counts())\n",
        "\n",
        "        # Performance metrics\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"\\nğŸš€ GPU PERFORMANCE:\")\n",
        "            print(f\"GPU Memory Used: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "            print(f\"GPU Memory Cached: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
        "\n",
        "    else:\n",
        "        print(\"âŒ No questions were successfully processed\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ No questions found to process\")\n",
        "\n",
        "print(f\"\\nğŸ¯ FIXED Process completed! Check the generated CSV files for results.\")\n",
        "print(f\"ğŸ“ Output files:\")\n",
        "print(f\"  - FIXED_processed_questions.csv (main results)\")\n",
        "print(f\"  - FIXED_unit_wise_topics.csv (unit-wise topics)\")\n",
        "\n",
        "# âœ… STEP 8: Memory cleanup\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"ğŸ§¹ GPU memory cleaned up\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ”§ KEY FIXES IMPLEMENTED:\")\n",
        "print(\"=\"*80)\n",
        "print(\"1. âœ… FIXED Question Number Removal:\")\n",
        "print(\"   - Removes '1.', 'Q1.', '1)', etc. from questions\")\n",
        "print(\"   - Clean questions without numbering artifacts\")\n",
        "print(\"\")\n",
        "print(\"2. âœ… ENHANCED Question Detection:\")\n",
        "print(\"   - Better patterns for both '?' and '.' endings\")\n",
        "print(\"   - Improved validation for command-style questions\")\n",
        "print(\"   - More comprehensive question indicators\")\n",
        "print(\"\")\n",
        "print(\"3. âœ… IMPROVED Text Processing:\")\n",
        "print(\"   - Better line filtering to skip headers/metadata\")\n",
        "print(\"   - Enhanced cleaning of question text\")\n",
        "print(\"   - Reduced false positives\")\n",
        "print(\"\")\n",
        "print(\"4. âœ… ROBUST Extraction Pipeline:\")\n",
        "print(\"   - Multiple extraction strategies\")\n",
        "print(\"   - Better OCR fallback\")\n",
        "print(\"   - Improved deduplication\")\n",
        "print(\"\")\n",
        "print(\"5. âœ… INTEGER MARKS with MINIMUM 3:\")\n",
        "print(\"   - All marks stored as integers (3, 4, 5, 6, 7, 8)\")\n",
        "print(\"   - Minimum mark value enforced to be 3\")\n",
        "print(\"   - No decimal marks or marks below 3\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS9EWQbhflR3",
        "outputId": "4394a16f-2b6f-4c25-d8ad-63c1dc294d6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.11)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrcljNDU-MNg",
        "outputId": "238dd14f-2c92-4701-c6e5-f2e3e115eb60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ Google Colab Document Processing Setup\n",
            "==================================================\n",
            "ğŸ”— Backend URL: https://068e-49-204-87-250.ngrok-free.app\n",
            "ğŸ”§ Colab Port: 8000\n",
            "ğŸ”‘ API Key: 8162079687...\n",
            "\n",
            "âš ï¸  IMPORTANT: Update the BACKEND_URL above to your actual backend URL\n",
            "âš ï¸  If your Node.js server is running locally, you need to expose it via ngrok\n",
            "\n",
            "ğŸ§ª Testing backend callback...\n",
            "ğŸ§ª Testing backend callback...\n",
            "ğŸ§ª Test callback response: 500\n",
            "ğŸ§ª Response text: {\"message\":\"Failed to update processing status\"}\n",
            "\n",
            "âš ï¸  SETUP INSTRUCTIONS:\n",
            "1. Make sure BACKEND_URL points to your actual backend\n",
            "2. Copy the ngrok URL that will be displayed below\n",
            "3. Update your backend .env file with the COLAB_WEBHOOK_URL\n",
            "\n",
            "ğŸš€ Setting up Google Colab integration...\n",
            "ğŸ”§ Using port 8000 for Colab server\n",
            "ğŸ”— Backend URL: https://068e-49-204-87-250.ngrok-free.app\n",
            "ğŸ§ª Testing backend connection...\n",
            "âœ… Backend is reachable!\n",
            "ğŸ”— Starting ngrok tunnel on port 8000...\n",
            "âœ… Webhook endpoint created: NgrokTunnel: \"https://55f5-34-59-96-172.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "ğŸ“ Update your backend .env file with:\n",
            "   COLAB_WEBHOOK_URL=NgrokTunnel: \"https://55f5-34-59-96-172.ngrok-free.app\" -> \"http://localhost:8000\"/process-documents\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Address already in use\n",
            "Port 8000 is in use by another program. Either identify and stop that program, or start the server with a different port.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ‰ Colab integration setup completed!\n",
            "ğŸ’¡ Your Colab notebook is now ready to receive processing requests\n",
            "ğŸ”„ Webhook is running and waiting for requests...\n",
            "ğŸ”— Webhook URL: NgrokTunnel: \"https://55f5-34-59-96-172.ngrok-free.app\" -> \"http://localhost:8000\"/process-documents\n",
            "ğŸ”— Health check: NgrokTunnel: \"https://55f5-34-59-96-172.ngrok-free.app\" -> \"http://localhost:8000\"/health\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¥ Received processing request!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [10/Jun/2025 09:46:32] \"POST /process-documents HTTP/1.1\" 200 -\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‹ Request data keys: ['processId', 'questionBank', 'syllabus', 'callbackUrl']\n",
            "ğŸ”‘ Process ID: 6847b19f5abf25eb87e91aab\n",
            "ğŸ“„ Question bank size: 11652 chars\n",
            "ğŸ“‹ Syllabus size: 1335780 chars\n",
            "ğŸ”„ Starting async processing for 6847b19f5abf25eb87e91aab\n",
            "ğŸ“ Temporary files saved\n",
            "ğŸ“„ Extracting questions...\n",
            "ğŸ” Starting FIXED question extraction...\n",
            "ğŸ“„ Page 1: Found 24 questions\n",
            "ğŸ“„ Page 2: Found 26 questions\n",
            "ğŸ“„ Page 3: Found 19 questions\n",
            "ğŸ“„ Page 4: Found 24 questions\n",
            "ğŸ“„ Page 5: Found 27 questions\n",
            "ğŸ“„ Page 6: Found 25 questions\n",
            "ğŸ“„ Page 7: Found 28 questions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“„ Page 8: Found 5 questions\n",
            "âœ… Text extraction found: 178 questions\n",
            "âœ… After deduplication: 178 unique questions\n",
            "ğŸ“‹ Extracting topics...\n",
            "\n",
            "ğŸ” Starting topic extraction for all units...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Using pattern 1: Found 5 units\n",
            "  Unit I: 7 topics\n",
            "  Unit II: 8 topics\n",
            "  Unit III: 11 topics\n",
            "  Unit IV: 8 topics\n",
            "  Unit V: 10 topics\n",
            "âœ… Found 178 questions\n",
            "ğŸ¤– Processing with ML models...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "âœ… Processing completed: 178 questions, 44 topics\n",
            "ğŸ“¤ Sending results to backend...\n",
            "ğŸ”— URL: https://068e-49-204-87-250.ngrok-free.app/api/upload/processing-complete\n",
            "ğŸ”‘ Process ID: 6847b19f5abf25eb87e91aab\n",
            "ğŸ“Š Questions: 178\n",
            "ğŸ“‹ Topics: 44\n",
            "âœ… Status: success\n",
            "ğŸ“¡ Making request to: https://068e-49-204-87-250.ngrok-free.app/api/upload/processing-complete\n",
            "ğŸ”‘ Using API key: 8162079687...\n",
            "ğŸ“¨ Response Status: 200\n",
            "ğŸ“¨ Response Text: {\"message\":\"Status updated successfully\"}\n",
            "âœ… Results sent successfully to backend!\n",
            "ğŸ—‘ï¸ Cleaned up temporary files\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¥ Received processing request!\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-4-2644ac5518a8>\", line 136, in process_documents\n",
            "    data = request.get_json()\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/wrappers/request.py\", line 608, in get_json\n",
            "    data = self.get_data(cache=cache)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/wrappers/request.py\", line 422, in get_data\n",
            "    rv = self.stream.read()\n",
            "         ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/wsgi.py\", line 577, in readall\n",
            "    data = self.read(1024 * 64)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/wsgi.py\", line 562, in readinto\n",
            "    self.on_disconnect()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/werkzeug/wsgi.py\", line 499, in on_disconnect\n",
            "    raise ClientDisconnected()\n",
            "werkzeug.exceptions.ClientDisconnected: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.\n",
            "INFO:werkzeug:127.0.0.1 - - [10/Jun/2025 10:14:58] \"\u001b[35m\u001b[1mPOST /process-documents HTTP/1.1\u001b[0m\" 500 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ Webhook error: 400 Bad Request: The browser (or proxy) sent a request that this server could not understand.\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n",
            "ğŸ“¡ Webhook still active and listening for requests...\n"
          ]
        }
      ],
      "source": [
        "# Fixed Google Colab Integration Code - Port Conflict Resolution\n",
        "# Replace your existing Colab code with this\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import pandas as pd\n",
        "from pyngrok import ngrok\n",
        "from flask import Flask, request, jsonify\n",
        "import threading\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Set your ngrok auth token\n",
        "ngrok.set_auth_token(\"2yE2JW29TGeZBmN0AIa5hSSMpdj_7cR52txvwnkCM3prHtZsR\")\n",
        "\n",
        "# âœ… FIXED: Configuration\n",
        "# Since your Node.js server is on port 5000, we'll use a different port for Colab\n",
        "COLAB_PORT = 8000  # Different port to avoid conflict\n",
        "BACKEND_URL = \"https://068e-49-204-87-250.ngrok-free.app\"  # âœ… Use your backend's ngrok URL\n",
        "API_KEY = \"81620796872873ca49ef90a37ac274bb62343c70a21f2045ad5a43b3ff9d9eb9\"\n",
        "\n",
        "# Flask app for receiving webhook requests\n",
        "app = Flask(__name__)\n",
        "\n",
        "def send_results_to_backend(process_id, questions, topics, status=\"success\", error_message=None):\n",
        "    \"\"\"\n",
        "    Send processing results back to your Node.js backend\n",
        "    âœ… FIXED: Use the correct backend URL\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # âœ… FIXED: Use your actual backend ngrok URL\n",
        "        callback_url = f\"{BACKEND_URL}/api/upload/processing-complete\"\n",
        "\n",
        "        print(f\"ğŸ“¤ Sending results to backend...\")\n",
        "        print(f\"ğŸ”— URL: {callback_url}\")\n",
        "        print(f\"ğŸ”‘ Process ID: {process_id}\")\n",
        "        print(f\"ğŸ“Š Questions: {len(questions) if questions else 0}\")\n",
        "        print(f\"ğŸ“‹ Topics: {len(topics) if topics else 0}\")\n",
        "        print(f\"âœ… Status: {status}\")\n",
        "\n",
        "        # Prepare payload\n",
        "        payload = {\n",
        "            'processId': process_id,\n",
        "            'status': status\n",
        "        }\n",
        "\n",
        "        if status == 'success':\n",
        "            payload['questions'] = questions\n",
        "            payload['topics'] = topics\n",
        "        else:\n",
        "            payload['error'] = error_message or 'Processing failed'\n",
        "\n",
        "        # âœ… FIXED: Use correct header format\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json',\n",
        "            'x-api-key': API_KEY,\n",
        "            'User-Agent': 'GoogleColab/1.0',\n",
        "            'ngrok-skip-browser-warning': 'true'  # Skip ngrok browser warning\n",
        "        }\n",
        "\n",
        "        print(f\"ğŸ“¡ Making request to: {callback_url}\")\n",
        "        print(f\"ğŸ”‘ Using API key: {API_KEY[:10]}...\")\n",
        "\n",
        "        # Send the request\n",
        "        response = requests.post(\n",
        "            callback_url,\n",
        "            json=payload,\n",
        "            headers=headers,\n",
        "            timeout=30\n",
        "        )\n",
        "\n",
        "        print(f\"ğŸ“¨ Response Status: {response.status_code}\")\n",
        "        print(f\"ğŸ“¨ Response Text: {response.text}\")\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            print(\"âœ… Results sent successfully to backend!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"âŒ Backend returned error: {response.status_code}\")\n",
        "            print(f\"âŒ Error details: {response.text}\")\n",
        "            return False\n",
        "\n",
        "    except requests.exceptions.ConnectionError as e:\n",
        "        print(f\"âŒ Connection error - check your backend URL: {str(e)}\")\n",
        "        return False\n",
        "    except requests.exceptions.Timeout as e:\n",
        "        print(f\"âŒ Timeout error: {str(e)}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error sending results to backend: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def test_backend_connection():\n",
        "    \"\"\"\n",
        "    Test if we can reach the backend server\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"ğŸ§ª Testing backend connection...\")\n",
        "\n",
        "        # Try to reach the health endpoint\n",
        "        test_url = f\"{BACKEND_URL}/health\"\n",
        "\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json',\n",
        "            'User-Agent': 'GoogleColab/1.0',\n",
        "            'ngrok-skip-browser-warning': 'true'\n",
        "        }\n",
        "\n",
        "        response = requests.get(test_url, headers=headers, timeout=10)\n",
        "\n",
        "        if response.status_code in [200, 404]:  # 404 is fine, means server is running\n",
        "            print(\"âœ… Backend is reachable!\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"âš ï¸ Backend returned: {response.status_code}\")\n",
        "            return True  # Still consider it reachable\n",
        "\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        print(\"âŒ Cannot reach backend - check the BACKEND_URL!\")\n",
        "        print(f\"âŒ Current backend URL: {BACKEND_URL}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Backend test inconclusive: {str(e)}\")\n",
        "        return True  # Assume it's fine\n",
        "\n",
        "@app.route('/process-documents', methods=['POST'])\n",
        "def process_documents():\n",
        "    \"\"\"\n",
        "    Main webhook endpoint to receive processing requests from your backend\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"ğŸ“¥ Received processing request!\")\n",
        "\n",
        "        data = request.get_json()\n",
        "        if not data:\n",
        "            print(\"âŒ No JSON data received\")\n",
        "            return jsonify({'error': 'No JSON data'}), 400\n",
        "\n",
        "        print(f\"ğŸ“‹ Request data keys: {list(data.keys())}\")\n",
        "\n",
        "        # Extract required data\n",
        "        process_id = data.get('processId')\n",
        "        question_bank_base64 = data.get('questionBank')\n",
        "        syllabus_base64 = data.get('syllabus')\n",
        "        callback_url = data.get('callbackUrl')  # This might not be used since we use our own\n",
        "\n",
        "        print(f\"ğŸ”‘ Process ID: {process_id}\")\n",
        "        print(f\"ğŸ“„ Question bank size: {len(question_bank_base64) if question_bank_base64 else 0} chars\")\n",
        "        print(f\"ğŸ“‹ Syllabus size: {len(syllabus_base64) if syllabus_base64 else 0} chars\")\n",
        "\n",
        "        if not all([process_id, question_bank_base64, syllabus_base64]):\n",
        "            error_msg = \"Missing required data: processId, questionBank, or syllabus\"\n",
        "            print(f\"âŒ {error_msg}\")\n",
        "            return jsonify({'error': error_msg}), 400\n",
        "\n",
        "        # Start processing in background thread\n",
        "        def process_async():\n",
        "            try:\n",
        "                print(f\"ğŸ”„ Starting async processing for {process_id}\")\n",
        "\n",
        "                # Save temporary files\n",
        "                question_bank_path = f'/tmp/question_bank_{process_id}.pdf'\n",
        "                syllabus_path = f'/tmp/syllabus_{process_id}.pdf'\n",
        "\n",
        "                # Decode and save files\n",
        "                with open(question_bank_path, 'wb') as f:\n",
        "                    f.write(base64.b64decode(question_bank_base64))\n",
        "\n",
        "                with open(syllabus_path, 'wb') as f:\n",
        "                    f.write(base64.b64decode(syllabus_base64))\n",
        "\n",
        "                print(\"ğŸ“ Temporary files saved\")\n",
        "\n",
        "                # âœ… Use your existing processing functions\n",
        "                print(\"ğŸ“„ Extracting questions...\")\n",
        "                questions = extract_questions_fixed(question_bank_path)\n",
        "\n",
        "                print(\"ğŸ“‹ Extracting topics...\")\n",
        "                unit_topics = extract_topics_all_units_final(syllabus_path)\n",
        "\n",
        "                if len(questions) > 0:\n",
        "                    print(f\"âœ… Found {len(questions)} questions\")\n",
        "\n",
        "                    # âœ… Use your existing ML processing\n",
        "                    print(\"ğŸ¤– Processing with ML models...\")\n",
        "                    prediction_results = predict_batch_gpu(questions, batch_size=32)\n",
        "                    topic_results = match_topics_batch_gpu(questions, batch_size=32)\n",
        "\n",
        "                    # Format results\n",
        "                    processed_questions = []\n",
        "                    for i, (question, (marks, bloom), (matched_topic, unit, similarity)) in enumerate(\n",
        "                        zip(questions, prediction_results, topic_results)\n",
        "                    ):\n",
        "                        difficulty = get_difficulty(marks)\n",
        "\n",
        "                        processed_questions.append({\n",
        "                            \"question_id\": i + 1,\n",
        "                            \"question\": question,\n",
        "                            \"predicted_marks\": int(marks),\n",
        "                            \"bloom_level\": bloom,\n",
        "                            \"difficulty\": difficulty,\n",
        "                            \"matched_topic\": matched_topic,\n",
        "                            \"matched_unit\": unit,\n",
        "                            \"topic_similarity\": round(float(similarity), 3)\n",
        "                        })\n",
        "\n",
        "                    # Format topics\n",
        "                    processed_topics = []\n",
        "                    topic_id = 1\n",
        "                    for unit, topics in unit_topics.items():\n",
        "                        for topic in topics:\n",
        "                            processed_topics.append({\n",
        "                                \"unit\": unit,\n",
        "                                \"topic_id\": topic_id,\n",
        "                                \"topic\": topic\n",
        "                            })\n",
        "                            topic_id += 1\n",
        "\n",
        "                    print(f\"âœ… Processing completed: {len(processed_questions)} questions, {len(processed_topics)} topics\")\n",
        "\n",
        "                    # Send results to backend\n",
        "                    success = send_results_to_backend(\n",
        "                        process_id,\n",
        "                        processed_questions,\n",
        "                        processed_topics,\n",
        "                        'success'\n",
        "                    )\n",
        "\n",
        "                    if not success:\n",
        "                        print(\"âŒ Failed to send results\")\n",
        "                else:\n",
        "                    print(\"âŒ No questions extracted\")\n",
        "                    send_results_to_backend(process_id, [], [], 'failed', 'No questions extracted')\n",
        "\n",
        "                # Cleanup\n",
        "                try:\n",
        "                    os.remove(question_bank_path)\n",
        "                    os.remove(syllabus_path)\n",
        "                    print(\"ğŸ—‘ï¸ Cleaned up temporary files\")\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Processing error: {str(e)}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "                # Send failure status\n",
        "                send_results_to_backend(process_id, [], [], 'failed', str(e))\n",
        "\n",
        "        # Start processing in background\n",
        "        thread = threading.Thread(target=process_async)\n",
        "        thread.daemon = True\n",
        "        thread.start()\n",
        "\n",
        "        return jsonify({\n",
        "            'status': 'processing_started',\n",
        "            'processId': process_id,\n",
        "            'message': 'Document processing started successfully'\n",
        "        }), 200\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Webhook error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    \"\"\"Health check endpoint\"\"\"\n",
        "    return jsonify({\n",
        "        \"status\": \"healthy\",\n",
        "        \"message\": \"Colab processing server is running\",\n",
        "        \"backend_url\": BACKEND_URL,\n",
        "        \"colab_port\": COLAB_PORT,\n",
        "        \"timestamp\": time.time()\n",
        "    })\n",
        "\n",
        "@app.route('/test', methods=['GET', 'POST'])\n",
        "def test_endpoint():\n",
        "    \"\"\"Test endpoint for debugging\"\"\"\n",
        "    return jsonify({\n",
        "        \"message\": \"Colab server is working\",\n",
        "        \"backend_url\": BACKEND_URL,\n",
        "        \"method\": request.method,\n",
        "        \"colab_port\": COLAB_PORT,\n",
        "        \"timestamp\": time.time()\n",
        "    })\n",
        "\n",
        "def start_colab_server():\n",
        "    \"\"\"\n",
        "    Start the Colab server with ngrok tunnel\n",
        "    âœ… FIXED: Use different port to avoid conflict\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"ğŸš€ Setting up Google Colab integration...\")\n",
        "        print(f\"ğŸ”§ Using port {COLAB_PORT} for Colab server\")\n",
        "        print(f\"ğŸ”— Backend URL: {BACKEND_URL}\")\n",
        "\n",
        "        # Test backend connection\n",
        "        backend_reachable = test_backend_connection()\n",
        "        if not backend_reachable:\n",
        "            print(\"âš ï¸  WARNING: Backend server not reachable!\")\n",
        "            print(\"âš ï¸  Make sure your Node.js server is running and accessible\")\n",
        "            print(f\"âš ï¸  Expected backend URL: {BACKEND_URL}\")\n",
        "\n",
        "        # Kill any existing ngrok tunnels\n",
        "        try:\n",
        "            ngrok.kill()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        print(f\"ğŸ”— Starting ngrok tunnel on port {COLAB_PORT}...\")\n",
        "\n",
        "        # âœ… FIXED: Use different port for ngrok\n",
        "        public_url = ngrok.connect(COLAB_PORT)\n",
        "\n",
        "        print(f\"âœ… Webhook endpoint created: {public_url}\")\n",
        "        print(f\"ğŸ“ Update your backend .env file with:\")\n",
        "        print(f\"   COLAB_WEBHOOK_URL={public_url}/process-documents\")\n",
        "\n",
        "        # âœ… FIXED: Start Flask app on different port\n",
        "        def run_flask():\n",
        "            app.run(host='0.0.0.0', port=COLAB_PORT, debug=False, use_reloader=False)\n",
        "\n",
        "        flask_thread = threading.Thread(target=run_flask)\n",
        "        flask_thread.daemon = True\n",
        "        flask_thread.start()\n",
        "\n",
        "        # Wait a moment for Flask to start\n",
        "        time.sleep(3)\n",
        "\n",
        "        print(\"ğŸ‰ Colab integration setup completed!\")\n",
        "        print(\"ğŸ’¡ Your Colab notebook is now ready to receive processing requests\")\n",
        "        print(\"ğŸ”„ Webhook is running and waiting for requests...\")\n",
        "        print(f\"ğŸ”— Webhook URL: {public_url}/process-documents\")\n",
        "        print(f\"ğŸ”— Health check: {public_url}/health\")\n",
        "\n",
        "        # Keep the main thread alive and show status\n",
        "        try:\n",
        "            while True:\n",
        "                time.sleep(30)\n",
        "                print(\"ğŸ“¡ Webhook still active and listening for requests...\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nğŸ›‘ Shutting down...\")\n",
        "            ngrok.kill()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Setup error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "def test_backend_callback():\n",
        "    \"\"\"\n",
        "    Test sending a callback to the backend\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"ğŸ§ª Testing backend callback...\")\n",
        "\n",
        "        test_payload = {\n",
        "            'processId': 'test_123',\n",
        "            'status': 'success',\n",
        "            'questions': [\n",
        "                {\n",
        "                    \"question_id\": 1,\n",
        "                    \"question\": \"Test question\",\n",
        "                    \"predicted_marks\": 5,\n",
        "                    \"bloom_level\": \"L2\",\n",
        "                    \"difficulty\": \"Medium\",\n",
        "                    \"matched_topic\": \"Test Topic\",\n",
        "                    \"matched_unit\": \"Unit 1\",\n",
        "                    \"topic_similarity\": 0.95\n",
        "                }\n",
        "            ],\n",
        "            'topics': [\n",
        "                {\n",
        "                    \"unit\": \"Unit 1\",\n",
        "                    \"topic_id\": 1,\n",
        "                    \"topic\": \"Test Topic\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        callback_url = f\"{BACKEND_URL}/api/upload/processing-complete\"\n",
        "\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json',\n",
        "            'x-api-key': API_KEY,\n",
        "            'User-Agent': 'GoogleColab/1.0',\n",
        "            'ngrok-skip-browser-warning': 'true'\n",
        "        }\n",
        "\n",
        "        response = requests.post(callback_url, json=test_payload, headers=headers, timeout=10)\n",
        "\n",
        "        print(f\"ğŸ§ª Test callback response: {response.status_code}\")\n",
        "        print(f\"ğŸ§ª Response text: {response.text}\")\n",
        "\n",
        "        return response.status_code == 200\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ğŸ§ª Test callback failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ğŸ”§ Google Colab Document Processing Setup\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"ğŸ”— Backend URL: {BACKEND_URL}\")\n",
        "    print(f\"ğŸ”§ Colab Port: {COLAB_PORT}\")\n",
        "    print(f\"ğŸ”‘ API Key: {API_KEY[:10]}...\")\n",
        "    print(\"\")\n",
        "\n",
        "    print(\"âš ï¸  IMPORTANT: Update the BACKEND_URL above to your actual backend URL\")\n",
        "    print(\"âš ï¸  If your Node.js server is running locally, you need to expose it via ngrok\")\n",
        "    print(\"\")\n",
        "\n",
        "    # Test backend callback\n",
        "    print(\"ğŸ§ª Testing backend callback...\")\n",
        "    test_backend_callback()\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"âš ï¸  SETUP INSTRUCTIONS:\")\n",
        "    print(\"1. Make sure BACKEND_URL points to your actual backend\")\n",
        "    print(\"2. Copy the ngrok URL that will be displayed below\")\n",
        "    print(\"3. Update your backend .env file with the COLAB_WEBHOOK_URL\")\n",
        "    print(\"\")\n",
        "\n",
        "    # Start the server\n",
        "    start_colab_server()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}